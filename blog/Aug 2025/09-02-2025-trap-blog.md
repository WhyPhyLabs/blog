---
title: TRAP Post-Training Framework
---
Introducing <a href="link needed">Three-Ring Asynchronous Pipelining (TRAP)</a>, WhyPhy's post-training framework, designed for reinforcement learning workloads. We constructed TRAP from the ground up to maximize efficiency, flexibility, and ease of use, creating a world-class training framework that’s optimized yet light, opinionated yet modular, and generally uncompromising across the board. 

In developing TRAP, we aimed to optimize efficiency while simultaneously minimizing bloat and maintaining optionality for users based on a wide array of possible preferences. To achieve this, we built a Rust-based core plugin engine and modularized the entire system on top. This enables the framework to remain light but customizable, as core pieces can be easily mixed and matched through the simple installation and uninstallation of a PyPi package.

While TRAP is modular, we provide an opinionated implementation out of the box. First, and most obviously, was removing all hardware support other than NVIDIA. Though TRAP’s modularity makes even this interchangeable, this change significantly reduces the code base without having the same effect on the potential user base. TRAP ships with <a href="https://github.com/sgl-project/sglang">SGLang</a> as its sole rollout engine. A combination of performance, ecosystem, and execution by the core team made it the clear winner. Similarly, we settled on a single training backend, opting to go with <a href="https://arxiv.org/abs/1909.08053">Megatron-LM</a>, the underlying logic being a copy and paste of the reasoning behind our decision to use SGLang. We augment Megatron with <a href="https://github.com/alibaba/Pai-Megatron-Patch">Pai-Megatron-Patch</a> as a bonus, instantly increasing the usable model set.  Lastly, we ship with a full embrace of FP8, taking advantage of modern stabilization techniques such as <a href="https://fengyao.notion.site/off-policy-rl">Truncated Importance Sampling</a> to reap significant throughput and memory gains throughout the entirety of the framework, allowing you to do more with less.

Three-Ring Asynchronous Pipelining, the optimization for which the trainer is named, is the essential ingredient in realizing best-in-class performance. Built by combining <a href="https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51">Agentica and Together’s One-Off Pipelining</a> with <a href="https://novasky-ai.notion.site/skyrl-v0">SkyRL’s asynchronous pipelining</a>, TRAP facilitates parallelization amongst rollout actors, across actors and trainers, and between models, environments, and reward calculations, fighting Amdahl’s Law at three different levels, plus adding partial rollouts for good measure. 

Sticking with our ethos of leaving no efficiency rock unturned, TRAP ships with <a href="https://arxiv.org/abs/2407.20143">ByteCheckpoint</a>, the most effective open-source checkpointing system. This provides well more than an order of magnitude reduction in runtime checkpoint stalls, and multiples of improvement in saving and loading times compared against any other open-source option openly available today.

Turning to the reinforcement learning run itself, TRAP’s out-of-the-box implementation leverages advanced curriculum learning modules to decrease convergence time and bolster final performance. We incorporated <a href="https://arxiv.org/abs/2504.05520">AdaRFT</a> to dynamically set rollout difficulty, ensuring proper calibration with the model’s current capabilities, simultaneously turning the dials on both convergence speed and final results. We also leverage <a href="https://arxiv.org/abs/2506.09016">SPEED-RL</a>, which mimics DAPO’s dynamic sampling mechanism, but does so during the rollout phase, adding an extra layer of protection in saving valuable compute time. 

In addition to TRAP’s optimizations, we integrated it with the Verifiers reinforcement learning environment library. In doing so, TRAP enables immediate access to <a href="https://github.com/willccbb/verifiers">Verifiers’</a> environment ecosystem, including  <a href="https://app.primeintellect.ai/dashboard/environments">Prime Intellect’s Environment Hub</a>, greatly reducing the need to reimplement environments from scratch and increasing end-to-end development velocity.

We're thrilled to open-source TRAP as WhyPhy’s first significant release to the Open Ecosystem. As WhyPhy's primary training framework, TRAP will continue to evolve, ensuring that all improvements are shared with the community. <a href="link needed">Explore and contribute to TRAP</a> today as we help push the boundaries of open-source training.

<a href="https://x.com/Big_Uppy">Spencer Garnets</a> and <a href="https://x.com/xsudoer">Aria Bagheri</a><br/>Co-Founders of WhyPhy Labs

<br/> 

#### Citations

```bibtex
@misc{sheng2024hybridflow
	url = {https://arxiv.org/abs/2409.19256v2},
	author = {Guangming Sheng and Chi Zhang and Zilingfeng Ye and Xibin Wu and Wang Zhang and Ru Zhang and Yanghua Peng and Haibin Lin and Chuan Wu},
	title = {HybridFlow: A Flexible and Efficient RLHF Framework},
	publisher = {arXiv},
	year = {2024}
}
```

```bibtex
@misc{sgl-project,
	url = {https://github.com/sgl-project/sglang},
	author = {sgl-project},
	title = {sglang},
	year = {2025}
}
```

```bibtex
@misc{megatron-lm,
	url = {https://arxiv.org/abs/1909.08053},
	author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
	title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
	publisher = {arXiv},
	year = {2019}
}
```

```bibtex
@misc{pai-megatron-patch,
	url = {https://github.com/alibaba/Pai-Megatron-Patch},
	author = {alibaba},
	title = {Pai-Megatron-Patch},
	year = {2025}
}
```

```bibtex
@misc{yao2025offpolicy,
	url = {https://fengyao.notion.site/off-policy-rl},
	author = {Yao, Feng and Liu, Liyuan and Zhang, Dinghuai and Dong, Chengyu and Shang, Jingbo and Gao, Jianfeng},
	title = {Your Efficient RL Framework Secretly Brings You Off-Policy RL Training},
	year = {2025}
}
```

```bibtex
@misc{deepcoder2025,
	url = {https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51},
	author = {Michael Luo and Sijun Tan and Roy Huang and Ameen Patel and Alpay Ariyak and Qingyang Wu and Xiaoxiang Shi and Rachel Xin and Colin Cai and Maurice Weber and Ce Zhang and Li Erran Li and Raluca Ada Popa and Ion Stoica},
	title = {DeepCoder: A Fully Open-Source 14B Coder at O3-mini Level},
	year = {2025}
}
```

```bibtex
@misc{cao2025skyrl,
	url = {https://novasky-ai.notion.site/skyrl-v0},
	author = {Shiyi Cao and Sumanth Hegde and Dacheng Li and Tyler Griggs and Shu Liu and Eric Tang and Jiayi Pan and Xingyao Wang and Akshay Malik and Graham Neubig and Kourosh Hakhamaneshi and Richard Liaw and Philipp Moritz and Matei Zaharia and Joseph E. Gonzalez and Ion Stoica},
	title = {SkyRL-v0: Train Real-World Long-Horizon Agents via Reinforcement Learning},
	year = {2025}
}
```

```bibtex
@misc{wan2024bytecheckpoint,
	url = {https://arxiv.org/abs/2407.20143},
	author = {Borui, Wan and Mingji, Han and Yiyao, Sheng and Yanghua, Peng and Haibin, Lin and Mofan, Zhang and Zhichao, Lai and Menghan, Yu and Junda, Zhang and Zuquan, Song and Xin, Liu and Chuan, Wu},
	title = {ByteCheckpoint: A Unified Checkpointing System for Large Foundation Model Development},
	publisher = {arXiv},
	year = {2025}
}
```

```bibtex
@misc{shi2025efficient,
	url = {https://arxiv.org/abs/2504.05520},
	author = {Taiwei Shi and Yiyang Wu and Linxin Song and Tianyi Zhou and Jieyu Zhao},
	title = {Efficient Reinforcement Finetuning via Adaptive Curriculum Learning},
	publisher = {arXiv},
	year = {2025}
}
```

```bibtex
@misc{zhang2025speedrlfastertrainingreasoning,
	url = {https://arxiv.org/abs/2506.09016},
	author = {Ruiqi Zhang and Daman Arora and Song Mei and Andrea Zanette},
	title = {SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning},
	publisher = {arXiv},
	year = {2025}
}
```

```bibtex
@misc{brown-verifiers-2025,
	url = {https://github.com/willccbb/verifiers},
	author = {William Brown},
	title = {Verifiers},
	year = {2025}
}
```
